{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-2-TP-RNN-28th-subject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCKQaeLOcT3f"
      },
      "source": [
        "# Notebook Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G31k3xp2cWwe"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HZ7OV8RcLBN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58d291ea-545b-4e5b-c17b-108faf21abc8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "import datetime\n",
        "%load_ext tensorboard\n",
        "!rm -rf ./logs/\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3hWXf1Dcc5B"
      },
      "source": [
        "# The LSTM Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNsJdPkVcbiH"
      },
      "source": [
        "class MyModel(Model):\n",
        "  \"\"\"\n",
        "  LSTM Model\n",
        "\n",
        "  Parameters\n",
        "  ---------------------------\n",
        "  num_classes: int\n",
        "  Number of classes to predict\n",
        "\n",
        "  Attributes\n",
        "  ----------------------------\n",
        "  lstm1: tf.keras.layers\n",
        "  First layer of model\n",
        "\n",
        "  lstm2: tf.keras.layers\n",
        "  Second layer of model\n",
        "  lstm3: tf.keras.layers\n",
        "  Third layer of model\n",
        "\n",
        "  dense_output: tf.keras.layers\n",
        "  Output layer of model\n",
        "  \"\"\"\n",
        "  def __init__(self, num_classes):\n",
        "    super(MyModel, self).__init__()\n",
        "\n",
        "    self.lstm1 = LSTM(units=512, return_sequences=True) # 64 is the units quantity. return_sequences must be True because we want return all the hidden states\n",
        "    self.lstm2 = LSTM(256, dropout=0.4, return_sequences=True) \n",
        "    self.lstm3 = LSTM(128, dropout=0.4) \n",
        "    self.dropout = Dropout(0.4)\n",
        "    self.dense_output = Dense(num_classes, activation='softmax')\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    \"\"\"\n",
        "    Call the layers and perform their operations on the input tensors\n",
        "    :param inputs:  Input tensor\n",
        "    :return:        Output tensor\n",
        "    \"\"\"\n",
        " \n",
        "    x = self.lstm1(inputs)\n",
        "    x = self.lstm2(x)\n",
        "    x = self.lstm3(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.dense_output(x)\n",
        "\n",
        "    return x  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJDD3loBcgbr"
      },
      "source": [
        "data = pd.read_csv('https://drive.google.com/uc?id=1uHzOe01O52giF-N8iw0e_biLziHkOcXU', encoding = 'utf-8', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvnzjGZUcjcT"
      },
      "source": [
        "labels=pd.read_csv('https://drive.google.com/uc?id=1jN92fUr6FXp9DESStZM_rCynWAS-_DLA', encoding = 'utf-8', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk2L5zbGcmG1"
      },
      "source": [
        "# Preparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22d5iYrvcn7v"
      },
      "source": [
        "## Spliting the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJrEANBfclDW"
      },
      "source": [
        "def split_data_subject(data, labels, subject): \n",
        "  # Get data from a subject, based on labels matrix.\n",
        "\n",
        "  y_test_subj = labels[labels[1] == subject]   # get the samples of subject defined\n",
        "  y_train_subj = labels[labels[1] != subject]  # get for train all the subjects except the subject for test\n",
        "\n",
        "  X_train_subj = data.copy()\n",
        "  X_train_subj = X_train_subj.drop(y_test_subj.index) # Delete from data test samples\n",
        "\n",
        "  X_test_subj = data.copy()\n",
        "  X_test_subj = X_test_subj.drop(y_train_subj.index)\n",
        "\n",
        "  y_test_subj = y_test_subj.iloc[:,0:1].values\n",
        "  y_train_subj = y_train_subj.iloc[:,0:1].values\n",
        "\n",
        "  y_test_subj = y_test_subj - 1   # because initially our label is between 1 to 17, and we need put it to 0 to 16.\n",
        "  y_train_subj = y_train_subj -1\n",
        "\n",
        "  return X_train_subj, X_test_subj, y_train_subj, y_test_subj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUTBdvLfcsHc"
      },
      "source": [
        "X_train_i28, X_test_i28, y_train_i28, y_test_i28 = split_data_subject(data, labels, 28) #Calling the function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFLArWgUc1mO"
      },
      "source": [
        "# Transforming the Input Shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSmyvlWLc26s"
      },
      "source": [
        "# Input shape: [samples, timesteps, feature]\n",
        "# Reshape of 1d array to 3d array \n",
        "arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
        "# Output will have 1 arrays that contains 4 arrays, each with 3 elements\n",
        "newarr = arr.reshape(1, 4, 3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXS9xlMVc58P"
      },
      "source": [
        "def change_shape(X_i, y_i):\n",
        "  #Adapting the data for training\n",
        "\n",
        "  X = X_i.copy().values\n",
        "  # print(X[0,0], X[0,151], X[0,302])\n",
        "\n",
        "  X_x = X[:, 0:151]\n",
        "  X_y = X[:, 151:302]\n",
        "  X_z = X[:, 302:453]\n",
        "\n",
        "  # dstack: stack arrays in sequence depth wise (along third axis) \n",
        "  X_new = np.dstack([X_x, X_y, X_z]) \n",
        "\n",
        "  encoded = tf.keras.utils.to_categorical(y_i) # one-hot-encoded\n",
        "\n",
        "  return X_new, encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cY7r2pDc8K0"
      },
      "source": [
        "# Working with the 28th subject"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiD18mtKc7tb"
      },
      "source": [
        "X_new_train_i28, encodedtrain_i28 = change_shape(X_train_i28, y_train_i28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNdXfGbBdEM_"
      },
      "source": [
        "X_new_test_i28, encodedtest_i28 = change_shape(X_test_i28, y_test_i28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzovA-30dKc6"
      },
      "source": [
        "# Classifying UniMiB-SHAR with a LSTM network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyot8KHrdIRk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef5e3944-a114-41b8-ba2e-18e2b2fdef87"
      },
      "source": [
        "#!rm -rf ./logs/\n",
        "\n",
        "# Subject 28\n",
        "\n",
        "# Setting Tensorboard\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") #directory\n",
        "\n",
        "callbacks = [tf.keras.callbacks.ModelCheckpoint('best', monitor= 'val_accuracy',\n",
        "                                                verbose=1, save_best_only=True),\n",
        "             tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1, \n",
        "                                            write_graph=True)]\n",
        "\n",
        "num_classes = 17 # We have 17 kinds of activities in UNIMIB-CHAR\n",
        "with tf.device('/device:GPU:0'):\n",
        "  model = MyModel(num_classes) # Instatiating\n",
        "  model.compile(optimizer= tf.keras.optimizers.RMSprop(), \n",
        "                loss= 'categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_new_train_i28, encodedtrain_i28, batch_size=512, epochs=42, validation_data=(X_new_test_i28, encodedtest_i28),verbose=1, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/42\n",
            " 1/23 [>.............................] - ETA: 3s - loss: 2.8347 - accuracy: 0.0762WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            " 2/23 [=>............................] - ETA: 7s - loss: 2.8236 - accuracy: 0.1348WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1803s vs `on_train_batch_end` time: 0.3517s). Check your callbacks.\n",
            "23/23 [==============================] - ETA: 0s - loss: 2.3026 - accuracy: 0.2215\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.31646, saving model to best\n",
            "23/23 [==============================] - 9s 410ms/step - loss: 2.3026 - accuracy: 0.2215 - val_loss: 1.8550 - val_accuracy: 0.3165\n",
            "Epoch 2/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 2.0738 - accuracy: 0.2596\n",
            "Epoch 00002: val_accuracy did not improve from 0.31646\n",
            "23/23 [==============================] - 9s 376ms/step - loss: 2.0738 - accuracy: 0.2596 - val_loss: 1.8417 - val_accuracy: 0.3070\n",
            "Epoch 3/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 1.9008 - accuracy: 0.3236\n",
            "Epoch 00003: val_accuracy improved from 0.31646 to 0.43354, saving model to best\n",
            "23/23 [==============================] - 9s 386ms/step - loss: 1.9008 - accuracy: 0.3236 - val_loss: 1.5613 - val_accuracy: 0.4335\n",
            "Epoch 4/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 1.6701 - accuracy: 0.4189\n",
            "Epoch 00004: val_accuracy improved from 0.43354 to 0.43671, saving model to best\n",
            "23/23 [==============================] - 9s 401ms/step - loss: 1.6701 - accuracy: 0.4189 - val_loss: 1.5684 - val_accuracy: 0.4367\n",
            "Epoch 5/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 1.5043 - accuracy: 0.4793\n",
            "Epoch 00005: val_accuracy improved from 0.43671 to 0.63291, saving model to best\n",
            "23/23 [==============================] - 9s 407ms/step - loss: 1.5043 - accuracy: 0.4793 - val_loss: 1.0371 - val_accuracy: 0.6329\n",
            "Epoch 6/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 1.3121 - accuracy: 0.5455\n",
            "Epoch 00006: val_accuracy improved from 0.63291 to 0.70570, saving model to best\n",
            "23/23 [==============================] - 9s 401ms/step - loss: 1.3121 - accuracy: 0.5455 - val_loss: 0.9230 - val_accuracy: 0.7057\n",
            "Epoch 7/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 1.1151 - accuracy: 0.6131\n",
            "Epoch 00007: val_accuracy did not improve from 0.70570\n",
            "23/23 [==============================] - 9s 388ms/step - loss: 1.1151 - accuracy: 0.6131 - val_loss: 1.0205 - val_accuracy: 0.5981\n",
            "Epoch 8/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 1.0224 - accuracy: 0.6487\n",
            "Epoch 00008: val_accuracy did not improve from 0.70570\n",
            "23/23 [==============================] - 9s 382ms/step - loss: 1.0224 - accuracy: 0.6487 - val_loss: 0.9616 - val_accuracy: 0.6487\n",
            "Epoch 9/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.8514 - accuracy: 0.7092\n",
            "Epoch 00009: val_accuracy did not improve from 0.70570\n",
            "23/23 [==============================] - 9s 379ms/step - loss: 0.8514 - accuracy: 0.7092 - val_loss: 1.7646 - val_accuracy: 0.4778\n",
            "Epoch 10/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.7860 - accuracy: 0.7296\n",
            "Epoch 00010: val_accuracy improved from 0.70570 to 0.81013, saving model to best\n",
            "23/23 [==============================] - 9s 381ms/step - loss: 0.7860 - accuracy: 0.7296 - val_loss: 0.6209 - val_accuracy: 0.8101\n",
            "Epoch 11/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.6764 - accuracy: 0.7665\n",
            "Epoch 00011: val_accuracy did not improve from 0.81013\n",
            "23/23 [==============================] - 9s 381ms/step - loss: 0.6764 - accuracy: 0.7665 - val_loss: 0.5838 - val_accuracy: 0.7880\n",
            "Epoch 12/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.6217 - accuracy: 0.7880\n",
            "Epoch 00012: val_accuracy did not improve from 0.81013\n",
            "23/23 [==============================] - 9s 384ms/step - loss: 0.6217 - accuracy: 0.7880 - val_loss: 0.7484 - val_accuracy: 0.7437\n",
            "Epoch 13/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.5401 - accuracy: 0.8134\n",
            "Epoch 00013: val_accuracy did not improve from 0.81013\n",
            "23/23 [==============================] - 9s 387ms/step - loss: 0.5401 - accuracy: 0.8134 - val_loss: 0.7165 - val_accuracy: 0.7785\n",
            "Epoch 14/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.4993 - accuracy: 0.8331\n",
            "Epoch 00014: val_accuracy did not improve from 0.81013\n",
            "23/23 [==============================] - 9s 388ms/step - loss: 0.4993 - accuracy: 0.8331 - val_loss: 0.5388 - val_accuracy: 0.8038\n",
            "Epoch 15/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.4705 - accuracy: 0.8416\n",
            "Epoch 00015: val_accuracy improved from 0.81013 to 0.84494, saving model to best\n",
            "23/23 [==============================] - 9s 392ms/step - loss: 0.4705 - accuracy: 0.8416 - val_loss: 0.4795 - val_accuracy: 0.8449\n",
            "Epoch 16/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.4292 - accuracy: 0.8533\n",
            "Epoch 00016: val_accuracy did not improve from 0.84494\n",
            "23/23 [==============================] - 9s 389ms/step - loss: 0.4292 - accuracy: 0.8533 - val_loss: 0.5451 - val_accuracy: 0.8418\n",
            "Epoch 17/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.4021 - accuracy: 0.8675\n",
            "Epoch 00017: val_accuracy improved from 0.84494 to 0.88608, saving model to best\n",
            "23/23 [==============================] - 9s 388ms/step - loss: 0.4021 - accuracy: 0.8675 - val_loss: 0.4508 - val_accuracy: 0.8861\n",
            "Epoch 18/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.3257 - accuracy: 0.8907\n",
            "Epoch 00018: val_accuracy did not improve from 0.88608\n",
            "23/23 [==============================] - 9s 386ms/step - loss: 0.3257 - accuracy: 0.8907 - val_loss: 0.5271 - val_accuracy: 0.8259\n",
            "Epoch 19/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.3223 - accuracy: 0.8957\n",
            "Epoch 00019: val_accuracy did not improve from 0.88608\n",
            "23/23 [==============================] - 9s 384ms/step - loss: 0.3223 - accuracy: 0.8957 - val_loss: 0.5744 - val_accuracy: 0.8608\n",
            "Epoch 20/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.3004 - accuracy: 0.9018\n",
            "Epoch 00020: val_accuracy did not improve from 0.88608\n",
            "23/23 [==============================] - 9s 384ms/step - loss: 0.3004 - accuracy: 0.9018 - val_loss: 0.4961 - val_accuracy: 0.8671\n",
            "Epoch 21/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.3173 - accuracy: 0.9043\n",
            "Epoch 00021: val_accuracy did not improve from 0.88608\n",
            "23/23 [==============================] - 9s 383ms/step - loss: 0.3173 - accuracy: 0.9043 - val_loss: 0.5675 - val_accuracy: 0.8513\n",
            "Epoch 22/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.9191\n",
            "Epoch 00022: val_accuracy did not improve from 0.88608\n",
            "23/23 [==============================] - 9s 385ms/step - loss: 0.2564 - accuracy: 0.9191 - val_loss: 0.5941 - val_accuracy: 0.8418\n",
            "Epoch 23/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2034 - accuracy: 0.9337\n",
            "Epoch 00023: val_accuracy did not improve from 0.88608\n",
            "23/23 [==============================] - 9s 387ms/step - loss: 0.2034 - accuracy: 0.9337 - val_loss: 0.5927 - val_accuracy: 0.8481\n",
            "Epoch 24/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.9237\n",
            "Epoch 00024: val_accuracy did not improve from 0.88608\n",
            "23/23 [==============================] - 9s 385ms/step - loss: 0.2526 - accuracy: 0.9237 - val_loss: 0.4934 - val_accuracy: 0.8797\n",
            "Epoch 25/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1784 - accuracy: 0.9440\n",
            "Epoch 00025: val_accuracy improved from 0.88608 to 0.89873, saving model to best\n",
            "23/23 [==============================] - 9s 387ms/step - loss: 0.1784 - accuracy: 0.9440 - val_loss: 0.4390 - val_accuracy: 0.8987\n",
            "Epoch 26/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9385\n",
            "Epoch 00026: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 386ms/step - loss: 0.1938 - accuracy: 0.9385 - val_loss: 0.4905 - val_accuracy: 0.8766\n",
            "Epoch 27/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1697 - accuracy: 0.9484\n",
            "Epoch 00027: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 386ms/step - loss: 0.1697 - accuracy: 0.9484 - val_loss: 0.4791 - val_accuracy: 0.8513\n",
            "Epoch 28/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.9472\n",
            "Epoch 00028: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 386ms/step - loss: 0.1790 - accuracy: 0.9472 - val_loss: 0.5382 - val_accuracy: 0.8766\n",
            "Epoch 29/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1465 - accuracy: 0.9551\n",
            "Epoch 00029: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 386ms/step - loss: 0.1465 - accuracy: 0.9551 - val_loss: 0.5667 - val_accuracy: 0.8639\n",
            "Epoch 30/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1595 - accuracy: 0.9506\n",
            "Epoch 00030: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 386ms/step - loss: 0.1595 - accuracy: 0.9506 - val_loss: 0.5753 - val_accuracy: 0.8829\n",
            "Epoch 31/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1303 - accuracy: 0.9612\n",
            "Epoch 00031: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 387ms/step - loss: 0.1303 - accuracy: 0.9612 - val_loss: 0.7247 - val_accuracy: 0.8354\n",
            "Epoch 32/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1498 - accuracy: 0.9545\n",
            "Epoch 00032: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 388ms/step - loss: 0.1498 - accuracy: 0.9545 - val_loss: 0.5409 - val_accuracy: 0.8481\n",
            "Epoch 33/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1196 - accuracy: 0.9640\n",
            "Epoch 00033: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 387ms/step - loss: 0.1196 - accuracy: 0.9640 - val_loss: 0.4722 - val_accuracy: 0.8639\n",
            "Epoch 34/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9700\n",
            "Epoch 00034: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 387ms/step - loss: 0.1029 - accuracy: 0.9700 - val_loss: 0.7623 - val_accuracy: 0.8386\n",
            "Epoch 35/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1132 - accuracy: 0.9653\n",
            "Epoch 00035: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 387ms/step - loss: 0.1132 - accuracy: 0.9653 - val_loss: 0.4758 - val_accuracy: 0.8734\n",
            "Epoch 36/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9701\n",
            "Epoch 00036: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 388ms/step - loss: 0.1004 - accuracy: 0.9701 - val_loss: 0.5071 - val_accuracy: 0.8861\n",
            "Epoch 37/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9711\n",
            "Epoch 00037: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 386ms/step - loss: 0.0948 - accuracy: 0.9711 - val_loss: 0.6321 - val_accuracy: 0.8829\n",
            "Epoch 38/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9722\n",
            "Epoch 00038: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 387ms/step - loss: 0.0953 - accuracy: 0.9722 - val_loss: 1.2311 - val_accuracy: 0.7215\n",
            "Epoch 39/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9722\n",
            "Epoch 00039: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 387ms/step - loss: 0.1062 - accuracy: 0.9722 - val_loss: 0.8233 - val_accuracy: 0.8544\n",
            "Epoch 40/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1060 - accuracy: 0.9712\n",
            "Epoch 00040: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 385ms/step - loss: 0.1060 - accuracy: 0.9712 - val_loss: 0.9073 - val_accuracy: 0.8291\n",
            "Epoch 41/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9806\n",
            "Epoch 00041: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 385ms/step - loss: 0.0682 - accuracy: 0.9806 - val_loss: 0.6767 - val_accuracy: 0.8608\n",
            "Epoch 42/42\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9759\n",
            "Epoch 00042: val_accuracy did not improve from 0.89873\n",
            "23/23 [==============================] - 9s 384ms/step - loss: 0.0869 - accuracy: 0.9759 - val_loss: 0.5366 - val_accuracy: 0.8576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5eaa303780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1G8XjwEddj2"
      },
      "source": [
        "# Viewing the Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k3Pr_tmdNeb"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh3gYpbge5Hp"
      },
      "source": [
        "predictions = model.predict(X_new_test_i28) # Checking the preditions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HRoBjw-e6ub"
      },
      "source": [
        "print(predictions[55])\n",
        "print(encodedtest_i28[55])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lChPF3E5e_cB"
      },
      "source": [
        "print(np.argmax(predictions[55]))\n",
        "print(np.argmax(encodedtest_i28[55]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuPINtPTfCAx"
      },
      "source": [
        "y_ints = [y.argmax() for y in encodedtest_i28]\n",
        "p_ints = [y.argmax() for y in predictions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bajiHdXkfDjb"
      },
      "source": [
        "print(y_ints)\n",
        "print(p_ints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bujMCqnjfGtu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}