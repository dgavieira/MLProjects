{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trabalho-Redes-Neurais-Convolutivas.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_vzA7hhJCBc"
      },
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "> Author: Diego Vieira"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6fXaTVfJKmG"
      },
      "source": [
        "# Notebook Summary\n",
        "\n",
        "1. Importing Libraries\n",
        "2. Building the LeNet-5 Architecture\n",
        "3. Preparing the Data\n",
        "  * Creating the Dataset\n",
        "    * Mounting the Drive\n",
        "    * Setting DataImageGenerator\n",
        "  * Classifying COVID-19 dataset with a Convolutional Neural Network\n",
        "4. Setting Callbacks\n",
        "5. Viewing the Results\n",
        "6. Implementing Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUlp_6gZLv5W"
      },
      "source": [
        "# 1. Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtoHbWa7Iy0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f43d5565-dc16-436c-e916-ea9240634c2e"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, BatchNormalization, AveragePooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import timeit\n",
        "%load_ext tensorboard\n",
        "!rm -rf ./logs/\n",
        "%tensorflow_version 2.x\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCcsg4UhNNIZ"
      },
      "source": [
        "# 2. Building the LeNet-5 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXyWzZvoNPac"
      },
      "source": [
        "class LeNet5(Model):\n",
        "    \"\"\"\n",
        "  LeNet-5 Model\n",
        "\n",
        "  Attributes\n",
        "  --------------------------------\n",
        "  conv1: tf.keras.layers\n",
        "    Convolutional layer of model\n",
        "\n",
        "  conv2: tf.keras.layers\n",
        "    Convolutional layer of model\n",
        "\n",
        "  max_pool: tf.keras.layers\n",
        "    MaxPooling layer of model\n",
        "\n",
        "  flatten: tf.keras.layers\n",
        "    Flatten layer of model\n",
        "\n",
        "  dense1: tf.keras.layers\n",
        "  dense2: tf.keras.layers\n",
        "    Dense layer of model\n",
        "\n",
        "  dense3: tf.keras.layers\n",
        "    Output layer of model\n",
        "\n",
        "  \"\"\"\n",
        "    \n",
        "    def __init__(self, reg=0):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "        :param num_classes:     Number of classes to predict from\n",
        "        \"\"\"\n",
        "        super(LeNet5, self).__init__()\n",
        "        # We will build the various layers composing LeNet-5:\n",
        "        self.conv1 = Conv2D(6, kernel_size=(5, 5), padding='same', activation='relu', activity_regularizer=l2(reg))\n",
        "        self.conv2 = Conv2D(16, kernel_size=(5, 5), activation='relu', activity_regularizer=l2(reg))\n",
        "        self.max_pool = MaxPooling2D(pool_size=(2, 2))\n",
        "        self.flatten = Flatten()\n",
        "        self.dense1 = Dense(120, activation='relu', activity_regularizer=l2(reg))\n",
        "        self.dense2 = Dense(84, activation='relu', activity_regularizer=l2(reg))\n",
        "        self.dense3 = Dense(1, activation='sigmoid', activity_regularizer=l2(reg))\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Call the layers and perform their operations on the input tensors\n",
        "        :param inputs:  Input tensor\n",
        "        :return:        Output tensor\n",
        "        \"\"\"\n",
        "        x = self.max_pool(self.conv1(inputs))        # 1st block\n",
        "        x = self.max_pool(self.conv2(x))             # 2nd block\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense3(self.dense2(self.dense1(x))) # dense layers\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5791Bm1UZCnW"
      },
      "source": [
        "# 3. Preparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I175pY0NZeEX"
      },
      "source": [
        "## Creating the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4AIuukLZjmA"
      },
      "source": [
        "### Mounting drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJYnb_esZFya",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09bea37b-24d0-46d7-8ac0-52586c286d14"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMnDKpGtaEoZ"
      },
      "source": [
        "### Setting ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNX_TPnwZqxT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "08886aeb-2eff-468b-84a6-1c263a72d9ac"
      },
      "source": [
        "batch_size = 128\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "rescale = 1./255\n",
        "data_dir = '/content/drive/My Drive/Datasets/SARS-Cov-2/SARS-Cov-2/'\n",
        "\n",
        "datagen_aug=tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=rescale,\n",
        "    shear_range=0.05,\n",
        "    zoom_range=0.05,\n",
        "    #horizontal_flip = True,\n",
        "    #brightness_range = [0.8,1.2],\n",
        "    rotation_range = 15,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05\n",
        ")\n",
        "\n",
        "datagen=tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=rescale\n",
        ")\n",
        "\n",
        "train_ds = datagen.flow_from_directory(\n",
        "    directory = data_dir + 'Train',\n",
        "    target_size=(img_height, img_width),\n",
        "    class_mode='binary',\n",
        "    color_mode=\"grayscale\",\n",
        "    batch_size=batch_size)\n",
        "\n",
        "train_ds_aug = datagen_aug.flow_from_directory(\n",
        "  directory = data_dir + 'Train',\n",
        "  target_size=(img_height, img_width),\n",
        "  class_mode='binary',\n",
        "  color_mode= \"grayscale\",\n",
        "  batch_size=batch_size)\n",
        "\n",
        "valid_ds = datagen.flow_from_directory(\n",
        "    directory = data_dir + 'Valid',\n",
        "    target_size=(img_height, img_width),\n",
        "    class_mode='binary',\n",
        "    color_mode=\"grayscale\",\n",
        "    batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1895 images belonging to 2 classes.\n",
            "Found 1895 images belonging to 2 classes.\n",
            "Found 451 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVzrBnK4hpiQ"
      },
      "source": [
        "## Classifying COVID-19 with a Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-Z-I13uhuo1"
      },
      "source": [
        "# Instantiate and compiling\n",
        "with tf.device('/device:GPU:0'):\n",
        "  model = LeNet5()\n",
        "  model.compile(optimizer = tf.keras.optimizers.Adamax(learning_rate=5e-4),\n",
        "                loss= 'binary_crossentropy',\n",
        "                metrics = ['accuracy',\n",
        "                           tf.keras.metrics.AUC(),\n",
        "                           tf.keras.metrics.Precision(),\n",
        "                           tf.keras.metrics.Recall()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TFZGwKdi-70"
      },
      "source": [
        "# 4. Setting Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stjkjxh_lu9k"
      },
      "source": [
        "> Setting Tensorboard logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQgoM_Hgi6qL"
      },
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") #directory\n",
        "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\") # for metrics\n",
        "file_writer.set_as_default()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kux1uBqBmQe3"
      },
      "source": [
        "callbacks = [\n",
        "    # Callback to interrupt the training if the validation loss (`val_loss`) stops improving for over 3 epochs:\n",
        "    tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss'),\n",
        "    # Callback to log the graph, losses and metrics into TensorBoard (saving log files in `./logs` directory):\n",
        "    tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1, write_graph=True),\n",
        "    tf.keras.callbacks.ModelCheckpoint('best_model', monitor='val_accuracy', verbose=1, save_best_only=True)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CycoMxRtwaE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "outputId": "b50a599d-7140-4c2a-8a44-7a0df17b01f0"
      },
      "source": [
        "STEP_SIZE_TRAIN=train_ds.n//train_ds.batch_size\n",
        "STEP_SIZE_VALID=valid_ds.n//valid_ds.batch_size\n",
        "\n",
        "model.fit_generator(generator=train_ds,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=valid_ds,\n",
        "                    validation_steps=STEP_SIZE_VALID,callbacks=callbacks,\n",
        "                    epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-3b52394b08c7>:8: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/10\n",
            " 1/14 [=>............................] - ETA: 0s - loss: 0.6918 - accuracy: 0.4844 - auc: 0.7064 - precision: 0.0000e+00 - recall: 0.0000e+00WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.7391 - accuracy: 0.5252 - auc: 0.5392 - precision: 0.5222 - recall: 0.4328\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.52604, saving model to best_model\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.7391 - accuracy: 0.5252 - auc: 0.5392 - precision: 0.5222 - recall: 0.4328 - val_loss: 0.6665 - val_accuracy: 0.5260 - val_auc: 0.7802 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.6432 - accuracy: 0.6287 - auc: 0.7219 - precision: 0.6409 - recall: 0.5752\n",
            "Epoch 00002: val_accuracy improved from 0.52604 to 0.69271, saving model to best_model\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.6432 - accuracy: 0.6287 - auc: 0.7219 - precision: 0.6409 - recall: 0.5752 - val_loss: 0.6244 - val_accuracy: 0.6927 - val_auc: 0.8148 - val_precision: 0.8155 - val_recall: 0.4590\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.5776 - accuracy: 0.8175 - auc: 0.8917 - precision: 0.8287 - recall: 0.7980\n",
            "Epoch 00003: val_accuracy improved from 0.69271 to 0.71875, saving model to best_model\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.5776 - accuracy: 0.8175 - auc: 0.8917 - precision: 0.8287 - recall: 0.7980 - val_loss: 0.5881 - val_accuracy: 0.7188 - val_auc: 0.7925 - val_precision: 0.7349 - val_recall: 0.6559\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.4909 - accuracy: 0.8359 - auc: 0.9033 - precision: 0.8400 - recall: 0.8286\n",
            "Epoch 00004: val_accuracy did not improve from 0.71875\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.4909 - accuracy: 0.8359 - auc: 0.9033 - precision: 0.8400 - recall: 0.8286 - val_loss: 0.5710 - val_accuracy: 0.7057 - val_auc: 0.7749 - val_precision: 0.7070 - val_recall: 0.6236\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.4343 - accuracy: 0.8398 - auc: 0.9044 - precision: 0.8398 - recall: 0.8369\n",
            "Epoch 00005: val_accuracy improved from 0.71875 to 0.72135, saving model to best_model\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.4343 - accuracy: 0.8398 - auc: 0.9044 - precision: 0.8398 - recall: 0.8369 - val_loss: 0.5585 - val_accuracy: 0.7214 - val_auc: 0.7952 - val_precision: 0.6636 - val_recall: 0.8090\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3828 - accuracy: 0.8653 - auc: 0.9248 - precision: 0.8590 - recall: 0.8746\n",
            "Epoch 00006: val_accuracy improved from 0.72135 to 0.75521, saving model to best_model\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.3828 - accuracy: 0.8653 - auc: 0.9248 - precision: 0.8590 - recall: 0.8746 - val_loss: 0.5299 - val_accuracy: 0.7552 - val_auc: 0.8175 - val_precision: 0.7633 - val_recall: 0.7049\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.8727 - auc: 0.9342 - precision: 0.8693 - recall: 0.8743\n",
            "Epoch 00007: val_accuracy improved from 0.75521 to 0.75781, saving model to best_model\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.3506 - accuracy: 0.8727 - auc: 0.9342 - precision: 0.8693 - recall: 0.8743 - val_loss: 0.5114 - val_accuracy: 0.7578 - val_auc: 0.8343 - val_precision: 0.7104 - val_recall: 0.8441\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3226 - accuracy: 0.8857 - auc: 0.9469 - precision: 0.8856 - recall: 0.8836\n",
            "Epoch 00008: val_accuracy improved from 0.75781 to 0.77865, saving model to best_model\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.3226 - accuracy: 0.8857 - auc: 0.9469 - precision: 0.8856 - recall: 0.8836 - val_loss: 0.4854 - val_accuracy: 0.7786 - val_auc: 0.8503 - val_precision: 0.7632 - val_recall: 0.7838\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.9027 - auc: 0.9580 - precision: 0.9119 - recall: 0.8889\n",
            "Epoch 00009: val_accuracy did not improve from 0.77865\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.2883 - accuracy: 0.9027 - auc: 0.9580 - precision: 0.9119 - recall: 0.8889 - val_loss: 0.5258 - val_accuracy: 0.7526 - val_auc: 0.8462 - val_precision: 0.6953 - val_recall: 0.8710\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.2776 - accuracy: 0.9123 - auc: 0.9602 - precision: 0.9061 - recall: 0.9175\n",
            "Epoch 00010: val_accuracy improved from 0.77865 to 0.79427, saving model to best_model\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.2776 - accuracy: 0.9123 - auc: 0.9602 - precision: 0.9061 - recall: 0.9175 - val_loss: 0.4763 - val_accuracy: 0.7943 - val_auc: 0.8566 - val_precision: 0.7739 - val_recall: 0.8191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9fb01d3320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0TKsmEYzuGp"
      },
      "source": [
        "# 5. Viewing the Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZUBHdSPuNbT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "outputId": "1a911e0b-9088-4fdc-a9ef-e0d03d29d429"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 909), started 0:59:39 ago. (Use '!kill 909' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPOuidcfz7di"
      },
      "source": [
        "# 6. Implementing Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucKrACjSzyle",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b6110b1-819f-421d-9fb7-3c03bf70d22d"
      },
      "source": [
        "# Underfitting\n",
        "# Instatiate and compiling\n",
        "with tf.device('/device:GPU:0'):\n",
        "  model_aug = LeNet5(reg = 2e-5) \n",
        "  model_aug.compile(optimizer= tf.keras.optimizers.Adamax(learning_rate=0.001), loss= 'binary_crossentropy', metrics=['accuracy', \n",
        "                                                                                                               tf.keras.metrics.AUC(),\n",
        "                                                                                                               tf.keras.metrics.Precision(),\n",
        "                                                                                                               tf.keras.metrics.Recall()]) #Strings or methods?\n",
        "# Setting Tensorboard\n",
        "!rm -rf ./logs/ \n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") #directory\n",
        "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\") #for metrics\n",
        "file_writer.set_as_default()\n",
        "\n",
        "#1000 images - batch_size = 200 1000/200 \n",
        "\n",
        "callbacks = [\n",
        "    # Callback to interrupt the training if the validation loss (`val_loss`) stops improving for over 3 epochs:\n",
        "   # tf.keras.callbacks.EarlyStopping(patience=25, monitor='val_loss'),\n",
        "    # Callback to log the graph, losses and metrics into TensorBoard (saving log files in `./logs` directory):\n",
        "    tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1, write_graph=True),\n",
        "    tf.keras.callbacks.ModelCheckpoint('best_model', monitor='val_accuracy', verbose=1, save_best_only=True)]\n",
        "\n",
        "STEP_SIZE_TRAIN=train_ds_aug.n//train_ds_aug.batch_size\n",
        "STEP_SIZE_VALID=valid_ds.n//valid_ds.batch_size\n",
        "\n",
        "model_aug.fit_generator(generator=train_ds_aug,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=valid_ds, #class_weight=class_weight,\n",
        "                    validation_steps=STEP_SIZE_VALID,callbacks=callbacks,\n",
        "                    epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.7586 - accuracy: 0.5829 - auc_1: 0.5745 - precision_1: 0.6921 - recall_1: 0.2806\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.59375, saving model to best_model\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.7586 - accuracy: 0.5829 - auc_1: 0.5745 - precision_1: 0.6921 - recall_1: 0.2806 - val_loss: 0.7001 - val_accuracy: 0.5938 - val_auc_1: 0.6908 - val_precision_1: 0.7547 - val_recall_1: 0.2186\n",
            "Epoch 2/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.6854 - accuracy: 0.6484 - auc_1: 0.7293 - precision_1: 0.7529 - recall_1: 0.4348\n",
            "Epoch 00002: val_accuracy improved from 0.59375 to 0.61979, saving model to best_model\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.6854 - accuracy: 0.6484 - auc_1: 0.7293 - precision_1: 0.7529 - recall_1: 0.4348 - val_loss: 0.6758 - val_accuracy: 0.6198 - val_auc_1: 0.6702 - val_precision_1: 0.7500 - val_recall_1: 0.3226\n",
            "Epoch 3/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.6497 - accuracy: 0.6706 - auc_1: 0.7521 - precision_1: 0.7418 - recall_1: 0.5148\n",
            "Epoch 00003: val_accuracy improved from 0.61979 to 0.67969, saving model to best_model\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.6497 - accuracy: 0.6706 - auc_1: 0.7521 - precision_1: 0.7418 - recall_1: 0.5148 - val_loss: 0.6196 - val_accuracy: 0.6797 - val_auc_1: 0.7565 - val_precision_1: 0.6398 - val_recall_1: 0.7418\n",
            "Epoch 4/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.5994 - accuracy: 0.7227 - auc_1: 0.7953 - precision_1: 0.7149 - recall_1: 0.7329\n",
            "Epoch 00004: val_accuracy improved from 0.67969 to 0.74479, saving model to best_model\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.5994 - accuracy: 0.7227 - auc_1: 0.7953 - precision_1: 0.7149 - recall_1: 0.7329 - val_loss: 0.5673 - val_accuracy: 0.7448 - val_auc_1: 0.8042 - val_precision_1: 0.7792 - val_recall_1: 0.6522\n",
            "Epoch 5/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.5406 - accuracy: 0.7617 - auc_1: 0.8317 - precision_1: 0.7603 - recall_1: 0.7680\n",
            "Epoch 00005: val_accuracy improved from 0.74479 to 0.78385, saving model to best_model\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.5406 - accuracy: 0.7617 - auc_1: 0.8317 - precision_1: 0.7603 - recall_1: 0.7680 - val_loss: 0.5290 - val_accuracy: 0.7839 - val_auc_1: 0.8361 - val_precision_1: 0.7637 - val_recall_1: 0.7765\n",
            "Epoch 6/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.5025 - accuracy: 0.7702 - auc_1: 0.8546 - precision_1: 0.7546 - recall_1: 0.7943\n",
            "Epoch 00006: val_accuracy did not improve from 0.78385\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.5025 - accuracy: 0.7702 - auc_1: 0.8546 - precision_1: 0.7546 - recall_1: 0.7943 - val_loss: 0.5169 - val_accuracy: 0.7682 - val_auc_1: 0.8436 - val_precision_1: 0.7433 - val_recall_1: 0.7722\n",
            "Epoch 7/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.4651 - accuracy: 0.8036 - auc_1: 0.8788 - precision_1: 0.7980 - recall_1: 0.8062\n",
            "Epoch 00007: val_accuracy did not improve from 0.78385\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.4651 - accuracy: 0.8036 - auc_1: 0.8788 - precision_1: 0.7980 - recall_1: 0.8062 - val_loss: 0.4809 - val_accuracy: 0.7578 - val_auc_1: 0.8945 - val_precision_1: 0.6787 - val_recall_1: 0.9286\n",
            "Epoch 8/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.4431 - accuracy: 0.8178 - auc_1: 0.8886 - precision_1: 0.8096 - recall_1: 0.8280\n",
            "Epoch 00008: val_accuracy improved from 0.78385 to 0.82292, saving model to best_model\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.4431 - accuracy: 0.8178 - auc_1: 0.8886 - precision_1: 0.8096 - recall_1: 0.8280 - val_loss: 0.4398 - val_accuracy: 0.8229 - val_auc_1: 0.8873 - val_precision_1: 0.8122 - val_recall_1: 0.8122\n",
            "Epoch 9/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.4063 - accuracy: 0.8393 - auc_1: 0.9119 - precision_1: 0.8275 - recall_1: 0.8575\n",
            "Epoch 00009: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.4063 - accuracy: 0.8393 - auc_1: 0.9119 - precision_1: 0.8275 - recall_1: 0.8575 - val_loss: 0.4642 - val_accuracy: 0.7917 - val_auc_1: 0.8773 - val_precision_1: 0.7488 - val_recall_1: 0.8610\n",
            "Epoch 10/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3825 - accuracy: 0.8574 - auc_1: 0.9220 - precision_1: 0.8515 - recall_1: 0.8632\n",
            "Epoch 00010: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.3825 - accuracy: 0.8574 - auc_1: 0.9220 - precision_1: 0.8515 - recall_1: 0.8632 - val_loss: 0.4701 - val_accuracy: 0.7891 - val_auc_1: 0.8702 - val_precision_1: 0.7475 - val_recall_1: 0.8343\n",
            "Epoch 11/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3801 - accuracy: 0.8483 - auc_1: 0.9203 - precision_1: 0.8421 - recall_1: 0.8555\n",
            "Epoch 00011: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.3801 - accuracy: 0.8483 - auc_1: 0.9203 - precision_1: 0.8421 - recall_1: 0.8555 - val_loss: 0.4415 - val_accuracy: 0.8073 - val_auc_1: 0.8856 - val_precision_1: 0.7843 - val_recall_1: 0.8421\n",
            "Epoch 12/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3724 - accuracy: 0.8551 - auc_1: 0.9233 - precision_1: 0.8550 - recall_1: 0.8531\n",
            "Epoch 00012: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.3724 - accuracy: 0.8551 - auc_1: 0.9233 - precision_1: 0.8550 - recall_1: 0.8531 - val_loss: 0.4766 - val_accuracy: 0.7656 - val_auc_1: 0.9099 - val_precision_1: 0.6798 - val_recall_1: 0.9503\n",
            "Epoch 13/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3484 - accuracy: 0.8761 - auc_1: 0.9336 - precision_1: 0.8669 - recall_1: 0.8857\n",
            "Epoch 00013: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.3484 - accuracy: 0.8761 - auc_1: 0.9336 - precision_1: 0.8669 - recall_1: 0.8857 - val_loss: 0.4231 - val_accuracy: 0.8047 - val_auc_1: 0.9101 - val_precision_1: 0.7425 - val_recall_1: 0.9202\n",
            "Epoch 14/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3432 - accuracy: 0.8738 - auc_1: 0.9353 - precision_1: 0.8670 - recall_1: 0.8780\n",
            "Epoch 00014: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.3432 - accuracy: 0.8738 - auc_1: 0.9353 - precision_1: 0.8670 - recall_1: 0.8780 - val_loss: 0.4380 - val_accuracy: 0.8151 - val_auc_1: 0.8921 - val_precision_1: 0.7656 - val_recall_1: 0.8791\n",
            "Epoch 15/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3307 - accuracy: 0.8840 - auc_1: 0.9398 - precision_1: 0.8792 - recall_1: 0.8883\n",
            "Epoch 00015: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 22s 2s/step - loss: 0.3307 - accuracy: 0.8840 - auc_1: 0.9398 - precision_1: 0.8792 - recall_1: 0.8883 - val_loss: 0.4182 - val_accuracy: 0.8151 - val_auc_1: 0.9088 - val_precision_1: 0.7543 - val_recall_1: 0.9259\n",
            "Epoch 16/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3260 - accuracy: 0.8755 - auc_1: 0.9416 - precision_1: 0.8746 - recall_1: 0.8746\n",
            "Epoch 00016: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 22s 2s/step - loss: 0.3260 - accuracy: 0.8755 - auc_1: 0.9416 - precision_1: 0.8746 - recall_1: 0.8746 - val_loss: 0.5216 - val_accuracy: 0.7708 - val_auc_1: 0.9259 - val_precision_1: 0.6838 - val_recall_1: 0.9894\n",
            "Epoch 17/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3162 - accuracy: 0.8778 - auc_1: 0.9451 - precision_1: 0.8758 - recall_1: 0.8798\n",
            "Epoch 00017: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.3162 - accuracy: 0.8778 - auc_1: 0.9451 - precision_1: 0.8758 - recall_1: 0.8798 - val_loss: 0.4119 - val_accuracy: 0.8047 - val_auc_1: 0.9082 - val_precision_1: 0.7455 - val_recall_1: 0.8962\n",
            "Epoch 18/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3087 - accuracy: 0.8817 - auc_1: 0.9481 - precision_1: 0.8744 - recall_1: 0.8901\n",
            "Epoch 00018: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.3087 - accuracy: 0.8817 - auc_1: 0.9481 - precision_1: 0.8744 - recall_1: 0.8901 - val_loss: 0.4075 - val_accuracy: 0.8229 - val_auc_1: 0.9115 - val_precision_1: 0.7545 - val_recall_1: 0.9222\n",
            "Epoch 19/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3104 - accuracy: 0.8896 - auc_1: 0.9467 - precision_1: 0.8837 - recall_1: 0.8949\n",
            "Epoch 00019: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.3104 - accuracy: 0.8896 - auc_1: 0.9467 - precision_1: 0.8837 - recall_1: 0.8949 - val_loss: 0.4273 - val_accuracy: 0.8099 - val_auc_1: 0.8991 - val_precision_1: 0.7533 - val_recall_1: 0.9096\n",
            "Epoch 20/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.2985 - accuracy: 0.8929 - auc_1: 0.9511 - precision_1: 0.8829 - recall_1: 0.9047\n",
            "Epoch 00020: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.2985 - accuracy: 0.8929 - auc_1: 0.9511 - precision_1: 0.8829 - recall_1: 0.9047 - val_loss: 0.4253 - val_accuracy: 0.8073 - val_auc_1: 0.9035 - val_precision_1: 0.7368 - val_recall_1: 0.9231\n",
            "Epoch 21/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.2946 - accuracy: 0.8919 - auc_1: 0.9519 - precision_1: 0.8850 - recall_1: 0.9001\n",
            "Epoch 00021: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.2946 - accuracy: 0.8919 - auc_1: 0.9519 - precision_1: 0.8850 - recall_1: 0.9001 - val_loss: 0.4803 - val_accuracy: 0.7760 - val_auc_1: 0.9105 - val_precision_1: 0.7004 - val_recall_1: 0.9524\n",
            "Epoch 22/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.8990 - auc_1: 0.9577 - precision_1: 0.8885 - recall_1: 0.9077\n",
            "Epoch 00022: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.2811 - accuracy: 0.8990 - auc_1: 0.9577 - precision_1: 0.8885 - recall_1: 0.9077 - val_loss: 0.4785 - val_accuracy: 0.7734 - val_auc_1: 0.9081 - val_precision_1: 0.6944 - val_recall_1: 0.9459\n",
            "Epoch 23/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.8964 - auc_1: 0.9548 - precision_1: 0.9010 - recall_1: 0.8919\n",
            "Epoch 00023: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.2872 - accuracy: 0.8964 - auc_1: 0.9548 - precision_1: 0.9010 - recall_1: 0.8919 - val_loss: 0.5793 - val_accuracy: 0.7396 - val_auc_1: 0.9265 - val_precision_1: 0.6464 - val_recall_1: 0.9945\n",
            "Epoch 24/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.2867 - accuracy: 0.9015 - auc_1: 0.9549 - precision_1: 0.8878 - recall_1: 0.9163\n",
            "Epoch 00024: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.2867 - accuracy: 0.9015 - auc_1: 0.9549 - precision_1: 0.8878 - recall_1: 0.9163 - val_loss: 0.4704 - val_accuracy: 0.7630 - val_auc_1: 0.9212 - val_precision_1: 0.6783 - val_recall_1: 0.9563\n",
            "Epoch 25/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.2776 - accuracy: 0.9004 - auc_1: 0.9583 - precision_1: 0.8961 - recall_1: 0.9062\n",
            "Epoch 00025: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.2776 - accuracy: 0.9004 - auc_1: 0.9583 - precision_1: 0.8961 - recall_1: 0.9062 - val_loss: 0.4555 - val_accuracy: 0.7865 - val_auc_1: 0.9133 - val_precision_1: 0.7102 - val_recall_1: 0.9405\n",
            "Epoch 26/100\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.9049 - auc_1: 0.9605 - precision_1: 0.9001 - recall_1: 0.9084\n",
            "Epoch 00026: val_accuracy did not improve from 0.82292\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.2704 - accuracy: 0.9049 - auc_1: 0.9605 - precision_1: 0.9001 - recall_1: 0.9084 - val_loss: 0.4559 - val_accuracy: 0.7891 - val_auc_1: 0.9107 - val_precision_1: 0.7143 - val_recall_1: 0.9409\n",
            "Epoch 27/100\n",
            " 4/14 [=======>......................] - ETA: 10s - loss: 0.2777 - accuracy: 0.9102 - auc_1: 0.9591 - precision_1: 0.8852 - recall_1: 0.9409"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqEoyT1k0D8U"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}